{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLI IR - Spazio vettoriale\n",
    "\n",
    "Librerie -->nltk e scikit-learn\n",
    "\n",
    "## CountVectorizer\n",
    "**CouterVectorizer** classe per convertire il testo in una matrice di token. Usa la tf per la pesatura dei token e \n",
    "produce un modello con matrici sparse di tipo Numpy. Usa il modello **bag of word**.\n",
    "Di default fa già \n",
    "- una tokenizzazione \n",
    "- la creazione del vocabolario\n",
    "- costruzione della matrice\n",
    "\n",
    "Il costruttore se usato senza argomenti usa le sue impostazioni di default che pero è possibile modificare passando funzioni custom.\n",
    "Fa test pre-processing ma non rimuove le stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "#documenti da rappresentare nello spazio vettoriale\n",
    "#ogni riga è un documento\n",
    "corpus=[\"Racing games\",\n",
    "        \"This document describes racing cars\",\n",
    "        \"This document is about video games in general\",\n",
    "        \"This is a nice racing video game\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per creare il modello usiamo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  cars  describes  document  game  games  general  in  is  nice  \\\n",
      "0      0     0          0         0     0      1        0   0   0     0   \n",
      "1      0     1          1         1     0      0        0   0   0     0   \n",
      "2      1     0          0         1     0      1        1   1   1     0   \n",
      "3      0     0          0         0     1      0        0   0   1     1   \n",
      "\n",
      "   racing  this  video  \n",
      "0       1     0      0  \n",
      "1       1     1      0  \n",
      "2       0     1      1  \n",
      "3       1     1      1  \n"
     ]
    }
   ],
   "source": [
    "mod_vect=vectorizer.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(mod_vect.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "che serve per creare il mdello dai documenti, quindi impara il vocabolario e lo trasforma in una matrice termini-documenti.\n",
    "\n",
    "**Valori di ritorno** --> matrice sparsa Scipy, dove le riche sono i documenti e le colonne gli index termo per rappresentare i documenti nello spazio vettoriale. Nelle intersezioni si ha la tf, cioè la frequenza con cui una parola appare in quel documento.\n",
    "\n",
    "**Parametri** --> lista dei documenti, iterabile\n",
    "\n",
    "Per apprendere dai dati solo le statistiche per creare il modello senza apprendere il modello, quindi senza effettuare una trasformazione dei dati trasformandoli in una forma matriciale, ma calcolare solo il vocabolario usiamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "['about' 'cars' 'describes' 'document' 'game' 'games' 'general' 'in' 'is'\n",
      " 'nice' 'racing' 'this' 'video']\n"
     ]
    }
   ],
   "source": [
    "stats=vectorizer.fit(corpus)\n",
    "print(stats) \n",
    "print(stats.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valori di ritorno** --> un oggetto CountVectorizer, ma con le informazioni per creare il modello\n",
    "\n",
    "**Parametri** --> lista dei documenti, iterabile\n",
    "\n",
    "Poi usando questo nuovo CountVectorizer possiamo creare il modello usando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  cars  describes  document  game  games  general  in  is  nice  \\\n",
      "0      0     0          0         0     0      1        0   0   0     0   \n",
      "1      0     1          1         1     0      0        0   0   0     0   \n",
      "2      1     0          0         1     0      1        1   1   1     0   \n",
      "3      0     0          0         0     1      0        0   0   1     1   \n",
      "\n",
      "   racing  this  video  \n",
      "0       1     0      0  \n",
      "1       1     1      0  \n",
      "2       0     1      1  \n",
      "3       1     1      1  \n"
     ]
    }
   ],
   "source": [
    "mtd=stats.transform(corpus)\n",
    "df = pd.DataFrame(mtd.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questa funzione è usata anche per poter trasformare i nuovi dati da inserire nel modello. In questo caso il modello non viene ricreato, ma il nuovo documento che viene inserito viene trasposto nello spazio vettoriale già esistente considerando i termini presenti nello spazio. termini che non sono mai stati visti non vengono inseriti, ma scartati.\n",
    "\n",
    "## Cosine Similarity\n",
    "è una funzione che calcola il coseno fra due vettori. In questo caso il coseno rappresenta la similarità fra due documenti.\n",
    "1. se passiamo alla funzione solo la matrice del modello calcola la cos_sim fra i documenti usati per addrestrare il modello;\n",
    "2. se passiamo alla funzione la matrice del modello e la query, la cos_sim è calcolata fra la query e ogni singolo documeto. In questo caso la query deve essere prima inserita nello spazio vettoriale con la  *fit_trasform()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarità Coseno tra i documenti:\n",
      "[[1.         0.31622777 0.25       0.28867513]\n",
      " [0.31622777 1.         0.31622777 0.36514837]\n",
      " [0.25       0.31622777 1.         0.4330127 ]\n",
      " [0.28867513 0.36514837 0.4330127  1.        ]]\n",
      "\n",
      "Similarità Coseno tra la query e i documenti:\n",
      "[[0.5        0.31622777 0.         0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "# Calcolare la similarità coseno tra tutti i documenti\n",
    "cos = cosine_similarity(mod_vect)\n",
    "print(\"Similarità Coseno tra i documenti:\")\n",
    "print(cos)\n",
    "print()\n",
    "\n",
    "# Aggiungere una query al modello\n",
    "query = vectorizer.transform([\"racing game\"])\n",
    "\n",
    "# Calcolare la similarità coseno tra la query e i documenti\n",
    "cos = cosine_similarity(query, mod_vect)\n",
    "print(\"Similarità Coseno tra la query e i documenti:\")\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLI IR - Tf-idf\n",
    "\n",
    "## TfidfVectorizer\n",
    "TfidfVectorizer è una classe che usa come metrica la tf-idf, funziona come il caso precedente.\n",
    "\n",
    "# MODELLI IR - Inverted Index\n",
    "\n",
    "Librerie --> gensim\n",
    "\n",
    "Si potrebbe fare anche con scikit-learn usando **HashingVectorizer** ma usa come metrica la tf. Con gensim usando **Corpora.Dictionary** mappiamo ogni parola del testo ad un ID univoco, quindi usando un modello BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<8 unique tokens: ['car', 'describ', 'document', 'race', 'game']...>\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import snowball\n",
    "import re\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    \"\"\"tokenization function\"\"\"\n",
    "    sw=stopwords.words('english')\n",
    "    stemmer=snowball.SnowballStemmer(language=\"english\")\n",
    "    tokens=word_tokenize(text)\n",
    "    pruned=[stemmer.stem(t.lower()) for t in tokens \\\n",
    "            if re.search(r\"^\\w\",t) and not t.lower() in sw]\n",
    "    return pruned\n",
    "\n",
    "documents=[\"This document describes racing cars\",\n",
    "        \"This document is about video games in general\",\n",
    "        \"This is a nice racing video game\"]\n",
    "\n",
    "texts=[my_tokenizer(d) for d in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "\n",
    "# doc2bow per convertire i docs in BoW \n",
    "bow_corpus=[dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities.SpaseMatrixSimilarity\n",
    "La funzione **`SparseMatrixSimilarity`** è una classe di Gensim utilizzata per calcolare la similarità tra una query e un insieme di documenti, rappresentati come matrici sparse (in formato **TF-IDF** o **Bag-of-Words**).\n",
    "\n",
    "La funzione **`SparseMatrixSimilarity`** accetta i seguenti parametri:\n",
    "\n",
    "### 1. **`corpus`** \n",
    "- **Descrizione**: La matrice sparsa che rappresenta il corpus di documenti.\n",
    "- **Tipo**: Una matrice sparsa, generalmente una rappresentazione **TF-IDF** o **Bag-of-Words**.\n",
    "- **Esempio**: `tfidf[bow_corpus]`\n",
    "- **Funzione**: Il corpus è una matrice che contiene la rappresentazione numerica di tutti i documenti, dove le righe sono i documenti e le colonne sono i termini del vocabolario. Ogni valore nella matrice rappresenta l'importanza di un termine in un dato documento.\n",
    "\n",
    "### 2. **`num_features`** \n",
    "- **Descrizione**: Il numero di **caratteristiche** (o dimensioni) nel vocabolario.\n",
    "- **Tipo**: Un intero che rappresenta la lunghezza del vocabolario.\n",
    "- **Esempio**: `len(dictionary)` (dove `dictionary` è il vocabolario che contiene tutte le parole uniche nei documenti).\n",
    "- **Funzione**: Specifica la dimensione del vocabolario, ossia il numero di parole uniche presenti nel tuo corpus. Questo parametro è necessario per indicare la dimensione della matrice sparsa.\n",
    "\n",
    "### 3. **`num_best`** \n",
    "- **Descrizione**: Il numero di documenti più simili da restituire.\n",
    "- **Tipo**: Un intero (predefinito è 10).\n",
    "- **Esempio**: `num_best=5`\n",
    "- **Funzione**: Limita il numero di risultati restituiti, mostrando solo i documenti più simili alla query. Se non specificato, restituirà tutti i documenti con le loro similarità coseno rispetto alla query.\n",
    "\n",
    "### 4. **`threshold`** \n",
    "- **Descrizione**: Una soglia di similarità minima. Se la similarità coseno tra un documento e la query è inferiore a questa soglia, il documento verrà escluso dai risultati.\n",
    "- **Tipo**: Un valore float tra 0 e 1 (predefinito è 0.0).\n",
    "- **Esempio**: `threshold=0.5`\n",
    "- **Funzione**: Filtra i documenti con una similarità inferiore alla soglia specificata. Se impostato a 0.5, solo i documenti con una similarità maggiore o uguale a 0.5 saranno restituiti.\n",
    "\n",
    "---\n",
    "\n",
    "La funzione **`SparseMatrixSimilarity`** restituisce un oggetto che può essere utilizzato per calcolare la similarità coseno tra una query e il corpus di documenti.\n",
    "\n",
    "### Ritorno:\n",
    "- **Tipo**: Un oggetto di tipo **`SparseMatrixSimilarity`**.\n",
    "- **Funzione**: L'oggetto restituito è utilizzato per calcolare la similarità tra un dato documento e il corpus di documenti. Quando si fornisce una query a questo oggetto, restituirà un array di similarità coseno tra la query e ogni documento nel corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.0000001 , 0.07613309, 0.07613309], dtype=float32), array([0.07613309, 1.        , 0.19339646], dtype=float32), array([0.07613309, 0.19339646, 1.        ], dtype=float32)]\n",
      "[(0, 0.17312077), (1, 0.21988432), (2, 0.43976864)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus],len(dictionary))\n",
    "print(list(index))\n",
    "\n",
    "#tokenizzazione della query\n",
    "query_document = my_tokenizer(\"racing games\")\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLI IR AVANZATI - N-grammi\n",
    "\n",
    "Per utilizzare gli **n-grammi** con il **`TfidfVectorizer`**, basta impostare il parametro `ngram_range` durante la creazione dell'oggetto. Questo parametro definisce la lunghezza degli n-grammi da considerare.\n",
    "\n",
    "```python\n",
    "ngram_range=(min_n, max_n)\n",
    "``` \n",
    "- min_n: la lunghezza minima degli n-grammi (incluso).\n",
    "- max_n: la lunghezza massima degli n-grammi (incluso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caratteristiche (ngrammi) del vocabolario:\n",
      "['car' 'describ' 'describ race' 'document' 'document describ'\n",
      " 'document video' 'game' 'kill' 'kill radio' 'nice' 'nice race' 'race'\n",
      " 'race car' 'race video' 'radio' 'radio star' 'star' 'tabl' 'tabl game'\n",
      " 'video' 'video game' 'video kill' 'video tabl']\n",
      "Similarità coseno tra la query e i documenti:\n",
      "[[0.         0.30389824 0.59923094 0.11299246]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import snowball\n",
    "import re\n",
    "\n",
    "def my_tokenizer(text):\n",
    "        sw = stopwords.words('english')  \n",
    "        stemmer = snowball.SnowballStemmer(language=\"english\")  \n",
    "        tokens = word_tokenize(text)\n",
    "        pruned = [stemmer.stem(t) for t in tokens if re.search(r\"^[a-zA-Z]\", t) and not t in sw]\n",
    "        return pruned\n",
    "\n",
    "# Inizializza il vettorizzatore considerando i bigrammi\n",
    "vectorizer = TfidfVectorizer(tokenizer=my_tokenizer, ngram_range=(1, 2))\n",
    "\n",
    "corpus = [\"This document describes racing cars\",\n",
    "          \"This document is about videos of table games\",\n",
    "          \"This is a nice racing video game\",\n",
    "          \"Video killed the radio star\"]\n",
    "\n",
    "# Allena il modello sul corpus\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "print(\"Caratteristiche (ngrammi) del vocabolario:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Esegui la trasformazione della query nel modello\n",
    "query = vectorizer.transform([\"video game\"])\n",
    "cos = cosine_similarity(query, model)\n",
    "print(\"Similarità coseno tra la query e i documenti:\")\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLI AVANZATI IR - LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarità tra il primo documento e tutti gli altri\n",
      "\n",
      "[1.0000001  0.97902197 0.9843578  0.9853968  0.99798703 0.10885489\n",
      " 0.13524207 0.68249047 0.19551672 0.11114562]\n",
      "similarità tra il sesto documento e tutti gli altri\n",
      "\n",
      "MatrixSimilarity<10 docs, 2 features>\n",
      "[('debian', -0.44348546949989864), ('releas', -0.39387184658100155), ('woodi', -0.3514242109573063), ('gentoo', -0.2958535529232634), ('fix', -0.28306005410237145), ('wine', -0.28306005410237134), ('open', -0.1741743715775465), ('sourc', -0.17417437157754645), ('softwar', -0.15920294729105175), ('databas', -0.14610110507158347)]\n",
      "[('dna', -0.5359729615542282), ('dolli', -0.4042679632122835), ('damag', -0.40273638947754564), ('chip', -0.18203378118963026), ('human', -0.18203378118963026), ('news', -0.18203378118963026), ('low-cost', -0.18203378118963026), ('introduc', -0.18203378118963026), ('genom', -0.17581616785376497), ('total', -0.17564030867034527)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import snowball\n",
    "import re\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    sw=stopwords.words('english')\n",
    "    stemmer=snowball.SnowballStemmer(language=\"english\")\n",
    "    tokens=word_tokenize(text)\n",
    "    pruned=[stemmer.stem(t.lower()) for t in tokens \\\n",
    "            if re.search(r\"^[a-zA-Z]\",t) and not t.lower() in sw]\n",
    "    return pruned\n",
    "\n",
    "documents=[\"Indian government goes for open source software\",\n",
    "\"Debian 3.0 Woody released\",\n",
    "\"Wine 2.0 released with fixes for Gentoo 1.4 and Debian 3.0\",\n",
    "\"gnuPOD released: iPOD on Linux… with GPLed software\",\n",
    "\"Gentoo servers running at open source mySQL database\",\n",
    "\"Dolly the sheep not totally identical clone\",\n",
    "\"DNA news: introduced low-cost human genome DNA chip\",\n",
    "\"Malaria-parasite genome database on the Web\",\n",
    "\"UK sets up genome bank to protect rare sheep breeds\",\n",
    "\"Dolly's DNA damaged\"]\n",
    "\n",
    "texts=[]\n",
    "for d in documents:\n",
    "    # creates an array of tokenized documents\n",
    "    texts.append(my_tokenizer(d))\n",
    "\n",
    "# Crea il dizionario per il corpus dei documenti\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Creazione del modello TF-IDF\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "# Crea il modello LSI, con 2 argomenti (topics)\n",
    "lsi_model = models.LsiModel(corpus_tfidf, num_topics=2, id2word=dictionary)\n",
    "\n",
    "# Crea un indice che facilita il calcolo delle similarità\n",
    "index = similarities.MatrixSimilarity(lsi_model[corpus_tfidf])\n",
    "\n",
    "# Stampa la similarità tra il primo documento e tutti gli altri\n",
    "print(\"similarità tra il primo documento e tutti gli altri\\n\")\n",
    "print(list(index)[0])\n",
    "print(\"similarità tra il sesto documento e tutti gli altri\\n\") \n",
    "print(list(index)[5])\n",
    "\n",
    "# Stampa il primo argomento del modello LSI\n",
    "print(lsi_model.show_topic(0))\n",
    "# Stampa il secondo argomento del modello LSI\n",
    "print(lsi_model.show_topic(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
