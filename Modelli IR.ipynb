{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLI IR - Spazio vettoriale\n",
    "\n",
    "Librerie -->nltk e scikit-learn\n",
    "\n",
    "## CountVectorizer\n",
    "**CouterVectorizer** classe per convertire il testo in una matrice di token. Usa la tf per la pesatura dei token e \n",
    "produce un modello con matrici sparse di tipo Numpy. Usa il modello **bag of word**.\n",
    "Di default fa già \n",
    "- una tokenizzazione \n",
    "- la creazione del vocabolario\n",
    "- costruzione della matrice\n",
    "\n",
    "Il costruttore se usato senza argomenti usa le sue impostazioni di default che pero è possibile modificare passando funzioni custom.\n",
    "Fa test pre-processing ma non rimuove le stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "#documenti da rappresentare nello spazio vettoriale\n",
    "#ogni riga è un documento\n",
    "corpus=[\"Racing games\",\n",
    "        \"This document describes racing cars\",\n",
    "        \"This document is about video games in general\",\n",
    "        \"This is a nice racing video game\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per creare il modello usiamo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  cars  describes  document  game  games  general  in  is  nice  \\\n",
      "0      0     0          0         0     0      1        0   0   0     0   \n",
      "1      0     1          1         1     0      0        0   0   0     0   \n",
      "2      1     0          0         1     0      1        1   1   1     0   \n",
      "3      0     0          0         0     1      0        0   0   1     1   \n",
      "\n",
      "   racing  this  video  \n",
      "0       1     0      0  \n",
      "1       1     1      0  \n",
      "2       0     1      1  \n",
      "3       1     1      1  \n"
     ]
    }
   ],
   "source": [
    "mod_vect=vectorizer.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(mod_vect.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "che serve per creare il mdello dai documenti, quindi impara il vocabolario e lo trasforma in una matrice termini-documenti.\n",
    "\n",
    "**Valori di ritorno** --> matrice sparsa Scipy, dove le riche sono i documenti e le colonne gli index termo per rappresentare i documenti nello spazio vettoriale. Nelle intersezioni si ha la tf, cioè la frequenza con cui una parola appare in quel documento.\n",
    "\n",
    "**Parametri** --> lista dei documenti, iterabile\n",
    "\n",
    "Per apprendere dai dati solo le statistiche per creare il modello senza apprendere il modello, quindi senza effettuare una trasformazione dei dati trasformandoli in una forma matriciale, ma calcolare solo il vocabolario usiamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "['about' 'cars' 'describes' 'document' 'game' 'games' 'general' 'in' 'is'\n",
      " 'nice' 'racing' 'this' 'video']\n"
     ]
    }
   ],
   "source": [
    "stats=vectorizer.fit(corpus)\n",
    "print(stats) \n",
    "print(stats.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valori di ritorno** --> un oggetto CountVectorizer, ma con le informazioni per creare il modello\n",
    "\n",
    "**Parametri** --> lista dei documenti, iterabile\n",
    "\n",
    "Poi usando questo nuovo CountVectorizer possiamo creare il modello usando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  cars  describes  document  game  games  general  in  is  nice  \\\n",
      "0      0     0          0         0     0      1        0   0   0     0   \n",
      "1      0     1          1         1     0      0        0   0   0     0   \n",
      "2      1     0          0         1     0      1        1   1   1     0   \n",
      "3      0     0          0         0     1      0        0   0   1     1   \n",
      "\n",
      "   racing  this  video  \n",
      "0       1     0      0  \n",
      "1       1     1      0  \n",
      "2       0     1      1  \n",
      "3       1     1      1  \n"
     ]
    }
   ],
   "source": [
    "mtd=stats.transform(corpus)\n",
    "df = pd.DataFrame(mtd.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questa funzione è usata anche per poter trasformare i nuovi dati da inserire nel modello. In questo caso il modello non viene ricreato, ma il nuovo documento che viene inserito viene trasposto nello spazio vettoriale già esistente considerando i termini presenti nello spazio. termini che non sono mai stati visti non vengono inseriti, ma scartati.\n",
    "\n",
    "## Cosine Similarity\n",
    "è una funzione che calcola il coseno fra due vettori. In questo caso il coseno rappresenta la similarità fra due documenti.\n",
    "1. se passiamo alla funzione solo la matrice del modello calcola la cos_sim fra i documenti usati per addrestrare il modello;\n",
    "2. se passiamo alla funzione la matrice del modello e la query, la cos_sim è calcolata fra la query e ogni singolo documeto. In questo caso la query deve essere prima inserita nello spazio vettoriale con la  *fit_trasform()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarità Coseno tra i documenti:\n",
      "[[1.         0.31622777 0.25       0.28867513]\n",
      " [0.31622777 1.         0.31622777 0.36514837]\n",
      " [0.25       0.31622777 1.         0.4330127 ]\n",
      " [0.28867513 0.36514837 0.4330127  1.        ]]\n",
      "\n",
      "Similarità Coseno tra la query e i documenti:\n",
      "[[0.5        0.31622777 0.         0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "# Calcolare la similarità coseno tra tutti i documenti\n",
    "cos = cosine_similarity(mod_vect)\n",
    "print(\"Similarità Coseno tra i documenti:\")\n",
    "print(cos)\n",
    "print()\n",
    "\n",
    "# Aggiungere una query al modello\n",
    "query = vectorizer.transform([\"racing game\"])\n",
    "\n",
    "# Calcolare la similarità coseno tra la query e i documenti\n",
    "cos = cosine_similarity(query, mod_vect)\n",
    "print(\"Similarità Coseno tra la query e i documenti:\")\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLI IR - Tf-idf\n",
    "\n",
    "## TfidfVectorizer\n",
    "TfidfVectorizer è una classe che usa come metrica la tf-idf, funziona come il caso precedente.\n",
    "\n",
    "# MODELLI IR - Inverted Index\n",
    "\n",
    "Librerie --> gensim\n",
    "\n",
    "Si potrebbe fare anche con scikit-learn usando **HashingVectorizer** ma usa come metrica la tf. Con gensim usando **Corpora.Dictionary** mappiamo ogni parola del testo ad un ID univoco, quindi usando un modello BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m similarities\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\matutils.py:1034\u001b[39m\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1.\u001b[39m - \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 & set2)) / \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlogsumexp\u001b[39m(x):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gensim\\_matutils.pyx:1\u001b[39m, in \u001b[36minit gensim._matutils\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import snowball\n",
    "import re\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    \"\"\"tokenization function\"\"\"\n",
    "    sw=stopwords.words('english')\n",
    "    stemmer=snowball.SnowballStemmer(language=\"english\")\n",
    "    tokens=word_tokenize(text)\n",
    "    pruned=[stemmer.stem(t.lower()) for t in tokens \\\n",
    "            if re.search(r\"^\\w\",t) and not t.lower() in sw]\n",
    "    return pruned\n",
    "\n",
    "documents=[\"This document describes racing cars\",\n",
    "        \"This document is about video games in general\",\n",
    "        \"This is a nice racing video game\"]\n",
    "\n",
    "texts=[my_tokenizer(d) for d in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "\n",
    "# doc2bow per convertire i docs in BoW \n",
    "bow_corpus=[dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities.SpaseMatrixSimilarity\n",
    "La funzione **`SparseMatrixSimilarity`** è una classe di Gensim utilizzata per calcolare la similarità tra una query e un insieme di documenti, rappresentati come matrici sparse (in formato **TF-IDF** o **Bag-of-Words**).\n",
    "\n",
    "La funzione **`SparseMatrixSimilarity`** accetta i seguenti parametri:\n",
    "\n",
    "### 1. **`corpus`** \n",
    "- **Descrizione**: La matrice sparsa che rappresenta il corpus di documenti.\n",
    "- **Tipo**: Una matrice sparsa, generalmente una rappresentazione **TF-IDF** o **Bag-of-Words**.\n",
    "- **Esempio**: `tfidf[bow_corpus]`\n",
    "- **Funzione**: Il corpus è una matrice che contiene la rappresentazione numerica di tutti i documenti, dove le righe sono i documenti e le colonne sono i termini del vocabolario. Ogni valore nella matrice rappresenta l'importanza di un termine in un dato documento.\n",
    "\n",
    "### 2. **`num_features`** \n",
    "- **Descrizione**: Il numero di **caratteristiche** (o dimensioni) nel vocabolario.\n",
    "- **Tipo**: Un intero che rappresenta la lunghezza del vocabolario.\n",
    "- **Esempio**: `len(dictionary)` (dove `dictionary` è il vocabolario che contiene tutte le parole uniche nei documenti).\n",
    "- **Funzione**: Specifica la dimensione del vocabolario, ossia il numero di parole uniche presenti nel tuo corpus. Questo parametro è necessario per indicare la dimensione della matrice sparsa.\n",
    "\n",
    "### 3. **`num_best`** \n",
    "- **Descrizione**: Il numero di documenti più simili da restituire.\n",
    "- **Tipo**: Un intero (predefinito è 10).\n",
    "- **Esempio**: `num_best=5`\n",
    "- **Funzione**: Limita il numero di risultati restituiti, mostrando solo i documenti più simili alla query. Se non specificato, restituirà tutti i documenti con le loro similarità coseno rispetto alla query.\n",
    "\n",
    "### 4. **`threshold`** \n",
    "- **Descrizione**: Una soglia di similarità minima. Se la similarità coseno tra un documento e la query è inferiore a questa soglia, il documento verrà escluso dai risultati.\n",
    "- **Tipo**: Un valore float tra 0 e 1 (predefinito è 0.0).\n",
    "- **Esempio**: `threshold=0.5`\n",
    "- **Funzione**: Filtra i documenti con una similarità inferiore alla soglia specificata. Se impostato a 0.5, solo i documenti con una similarità maggiore o uguale a 0.5 saranno restituiti.\n",
    "\n",
    "---\n",
    "\n",
    "La funzione **`SparseMatrixSimilarity`** restituisce un oggetto che può essere utilizzato per calcolare la similarità coseno tra una query e il corpus di documenti.\n",
    "\n",
    "### Ritorno:\n",
    "- **Tipo**: Un oggetto di tipo **`SparseMatrixSimilarity`**.\n",
    "- **Funzione**: L'oggetto restituito è utilizzato per calcolare la similarità tra un dato documento e il corpus di documenti. Quando si fornisce una query a questo oggetto, restituirà un array di similarità coseno tra la query e ogni documento nel corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus],len(dictionary))\n",
    "print(list(index))\n",
    "\n",
    "#tokenizzazione della query\n",
    "query_document = my_tokenizer(\"racing games\")\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
